{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23d5770b",
   "metadata": {},
   "source": [
    "# Project MedVision: Radiology AI Assistant Lab\n",
    "\n",
    "Welcome back! In Notebook 01 you built a structured patient chart and crafted a handover note.\n",
    "Today you will hand that context to our imaging AI engine and evaluate how well it assists the care team."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaa1221",
   "metadata": {},
   "source": "## Mission Brief\n\n- Initialise the MedVision AI tooling inside Colab using the same self-contained setup pattern.\n- Reuse the patient summary you assembled earlier to ground every imaging prompt.\n- Experiment with prompts, personas, and images while logging clinical insights.\n- Benchmark MedGemma against an experimental LLaVA engine and make a deployment recommendation.\n\n### Why This Lab Matters: The Future of Diagnostic Collaboration\n\nRadiology AI is already deployed in hospitals worldwide for:\n- **Triage:** Flagging critical findings like pneumothorax for urgent reads\n- **Quality assurance:** Catching missed findings on \"normal\" reports  \n- **Workflow optimization:** Pre-populating draft reports for radiologist refinement\n\n**Your generation will inherit these tools.** This lab teaches you to:\n- Operate AI systems with clinical discipline\n- Evaluate outputs with appropriate skepticism\n- Communicate findings that blend AI insights with human judgment\n\n**The Goal:** Not to replace radiologists, but to help them focus expertise where it matters most while maintaining safety standards you'd want for your own family.\n\n### Learning Objectives\n\nBy the end of this lab, you will:\n1. Understand how vision-language models \"see\" and \"interpret\" medical images\n2. Craft effective prompts that ground AI analysis in clinical context\n3. Recognize AI hallucinations and apply verification protocols\n4. Evaluate competing AI models using clinical decision criteria\n5. Integrate AI-assisted reads into safe clinical workflows\n\n**Estimated time:** 90-120 minutes"
  },
  {
   "cell_type": "markdown",
   "id": "0b75dd59",
   "metadata": {},
   "source": [
    "### How to Use This Notebook\n",
    "- Run cells from top to bottom; restart the runtime if you change dependencies.\n",
    "- Challenge cells intentionally raise `NotImplementedError` until you complete them‚Äîthis keeps the workflow honest.\n",
    "- Solutions live in collapsed cells. Treat them as a safety net, not a shortcut.\n",
    "- Keep the handover note from Notebook 01 close by; you will reference it throughout the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bbffb7",
   "metadata": {},
   "source": [
    "## 0. Setup: Provision Your Colab Runtime\n",
    "\n",
    "This cell clones (or updates) the MedVision repository, installs dependencies, and ensures the toolkit is importable."
   ]
  },
  {
   "cell_type": "code",
   "id": "4fa0e8f1",
   "metadata": {},
   "source": "from __future__ import annotations\n\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import Final\n\nREPO_URL: Final[str] = \"https://github.com/your-repo/python-for-medicine-bootcamp.git\"\n\nworkspace = Path.cwd()\n\n# Try to find the project root\n# Case 1: Running from project root (workspace contains src/medvision_toolkit)\n# Case 2: Running from notebooks/ subdirectory (parent contains src/medvision_toolkit)\n# Case 3: Running in Colab (need to clone repository)\n\nif (workspace / \"src\" / \"medvision_toolkit\").exists():\n    repo_root = workspace\n    print(\"‚úì Detected Project MedVision source in the current workspace.\")\nelif (workspace.parent / \"src\" / \"medvision_toolkit\").exists():\n    repo_root = workspace.parent\n    print(\"‚úì Detected Project MedVision source in parent directory.\")\n    print(f\"  Project root: {repo_root}\")\nelse:\n    # Colab or git clone scenario\n    repo_root = workspace / \"python-for-medicine-bootcamp\"\n    if repo_root.exists():\n        print(\"Existing checkout found. Pulling latest changes...\")\n        result = subprocess.run(\n            [\"git\", \"-C\", str(repo_root), \"pull\", \"--ff-only\"],\n            capture_output=True,\n            text=True,\n        )\n        if result.returncode != 0:\n            print(result.stdout)\n            print(result.stderr, file=sys.stderr)\n            raise RuntimeError(\"git pull failed; please resolve and rerun the cell.\")\n    else:\n        print(f\"Cloning repository from {REPO_URL} ‚Ä¶\")\n        result = subprocess.run(\n            [\"git\", \"clone\", \"--depth\", \"1\", REPO_URL, str(repo_root)],\n            capture_output=True,\n            text=True,\n        )\n        if result.returncode != 0:\n            print(result.stdout)\n            print(result.stderr, file=sys.stderr)\n            raise RuntimeError(\"git clone failed; check the URL or your connection.\")\n\n# Install dependencies (only in Colab or if explicitly needed)\nrequirements_path = repo_root / \"requirements.txt\"\nif not requirements_path.exists():\n    raise FileNotFoundError(f\"Could not locate requirements.txt at {requirements_path}\")\n\n# Check if we're in Colab (has google.colab module)\ntry:\n    import google.colab\n    in_colab = True\nexcept ImportError:\n    in_colab = False\n\nif in_colab:\n    print(\"Installing Python dependencies (this may take a minute)‚Ä¶\")\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", str(requirements_path)])\nelse:\n    print(\"Running locally. Assuming dependencies are already installed.\")\n    print(\"If you encounter import errors, run: pip install -r requirements.txt\")\n\n# Add src directory to path\nproject_src = repo_root / \"src\"\nif str(project_src) not in sys.path:\n    sys.path.insert(0, str(project_src))\n    print(f\"‚úì Added {project_src} to Python path\")\n\nprint(\"‚úÖ Toolkit ready. Continue to the next cell.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "34838fc0",
   "metadata": {},
   "source": [
    "### Quick Systems Check\n",
    "\n",
    "Confirm the toolkit loads, summarise the patient profile, and preview tonight's handover so the AI work stays grounded in clinical reality."
   ]
  },
  {
   "cell_type": "code",
   "id": "f88a7bcc",
   "metadata": {},
   "source": [
    "from medvision_toolkit import (\n",
    "    draft_handover_for_night_team,\n",
    "    load_sample_patient,\n",
    "    summarize_patient,\n",
    ")\n",
    "\n",
    "patient_profile = load_sample_patient()\n",
    "print(summarize_patient(patient_profile))\n",
    "\n",
    "handover_note = draft_handover_for_night_team(patient_profile)\n",
    "print(\"\\nNight Handover Preview:\\n\")\n",
    "print(handover_note)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "z8dpq8etvdf",
   "source": [
    "## 0.5. Before We Start: Understanding Medical AI\n",
    "\n",
    "Before you interact with the RadiologyAI engine, you need to understand what you're working with‚Äîand more importantly, what its limitations are.\n",
    "\n",
    "### Vision-Language Models: The Technology Behind the Assistant\n",
    "\n",
    "The RadiologyAI engine you're about to use is a **vision-language model (VLM)**‚Äîan AI system trained to:\n",
    "\n",
    "1. **\"See\"** patterns in images (like infiltrates, device placement, cardiac silhouettes)\n",
    "2. **\"Understand\"** your clinical questions in natural language  \n",
    "3. **\"Respond\"** with structured text findings\n",
    "\n",
    "**Key Insight:** Unlike rule-based systems or clinical decision tools, VLMs use **statistical pattern recognition**. They don't \"know\" medicine in the way a radiologist does‚Äîthey've learned associations from analyzing millions of images and text pairs during training.\n",
    "\n",
    "Think of it like this:\n",
    "- A radiologist **understands** that hazy opacities in a certain distribution suggest pulmonary edema because they know cardiac physiology\n",
    "- A VLM has **learned** that images with those visual patterns are often described using words like \"pulmonary edema\" in radiology reports\n",
    "\n",
    "The VLM can be remarkably helpful, but it's pattern-matching, not reasoning.\n",
    "\n",
    "### What is GGUF? Why Does It Matter in Colab?\n",
    "\n",
    "You'll see references to \"GGUF\" throughout this lab. Here's what you need to know:\n",
    "\n",
    "- **GGUF** = A quantized model format that trades some numerical precision for much smaller file sizes\n",
    "- **Full MedGemma model:** ~10GB (won't fit in free Colab's memory)\n",
    "- **GGUF quantized version:** ~4GB (perfect for teaching environments)\n",
    "- **The trade-off:** Slightly less nuanced outputs, but 95% as capable for learning purposes\n",
    "\n",
    "**Why this matters:** Quantization makes cutting-edge AI accessible for education and prototyping. In production radiology departments, you'd typically use the full-precision model on dedicated GPU servers.\n",
    "\n",
    "### The Three Pillars of Safe AI-Assisted Radiology\n",
    "\n",
    "‚ö†Ô∏è **CRITICAL SAFETY PRINCIPLES**\n",
    "\n",
    "Every AI-generated finding in this lab is **preliminary** and requires:\n",
    "\n",
    "1. **Verification:** A qualified radiologist must review the source images and confirm or revise AI-suggested findings\n",
    "2. **Clinical Correlation:** AI outputs must be interpreted in the context of patient history, physical exam, labs, and prior imaging\n",
    "3. **Documentation:** Any clinical decision informed by AI must document that AI was used as a decision support tool\n",
    "\n",
    "**In Real Practice:** You would never act on an AI read alone. Think of the AI as a smart intern's draft report‚Äîuseful for catching things you might have missed, but you always verify their work before signing off.\n",
    "\n",
    "**Scope of Practice:** As medical students and junior doctors, you are learning to consume AI outputs critically. Interpreting imaging studies and making diagnostic conclusions remains the domain of qualified radiologists and supervising physicians.\n",
    "\n",
    "### Common AI Failure Modes You'll Encounter\n",
    "\n",
    "**1. Hallucinations:** The AI confidently reports findings that aren't actually present in the image\n",
    "\n",
    "**2. Overconfidence:** Stating definitive diagnoses when findings are subtle or ambiguous  \n",
    "\n",
    "**3. Measurement Errors:** Quantifying things (angles, volumes, dimensions) without actual measurement tools\n",
    "\n",
    "**4. Context Blindness:** Missing clinically relevant details because the prompt didn't explicitly ask about them\n",
    "\n",
    "**5. Rare Finding Struggles:** Performing poorly on uncommon pathologies not well-represented in training data\n",
    "\n",
    "You'll practice detecting these failure modes throughout the lab.\n",
    "\n",
    "### Reflection Question (Think Before Moving On)\n",
    "\n",
    "**Scenario:** An overnight AI system flags a chest X-ray as \"likely normal\" and deprioritizes it in the reading queue. The next morning, a radiologist reviews it and identifies a small pneumothorax that the AI missed.\n",
    "\n",
    "Who is responsible? What safeguards should be in place?\n",
    "\n",
    "*There's no single right answer, but thinking through this now will make you a better steward of these tools.*\n",
    "\n",
    "---\n",
    "\n",
    "**Now that you understand what you're working with‚Äîand what could go wrong‚Äîlet's begin the lab with clear eyes.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "6b98e10e",
   "metadata": {},
   "source": [
    "## 1. Onboarding: Spin Up the Radiology Engine\n",
    "\n",
    "We abstracted the heavy lifting into helper functions. Initialising the engine should feel like calling a standard protocol."
   ]
  },
  {
   "cell_type": "code",
   "id": "1c3b0c1f",
   "metadata": {},
   "source": [
    "from medvision_toolkit import (\n",
    "    build_radiology_prompt,\n",
    "    initialize_medgemma_engine,\n",
    "    load_and_display_image,\n",
    "    render_ai_report,\n",
    ")\n",
    "\n",
    "print(\"--- Initialising MedGemma GGUF Engine ---\")\n",
    "medgemma_engine = initialize_medgemma_engine(backend=\"gguf\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4k3ophj1mz6",
   "source": [
    "### What Happens During Initialization?\n",
    "\n",
    "When you run the next cell, the toolkit will execute several steps behind the scenes:\n",
    "\n",
    "**Step 1: Model Download** (~3-5 minutes on first run)\n",
    "- The GGUF-quantized MedGemma model (~4GB) downloads from Hugging Face Hub\n",
    "- Subsequent runs are instant‚Äîthe model is cached in Colab's storage\n",
    "- If you restart the runtime, the cache persists for this session\n",
    "\n",
    "**Step 2: Model Loading** (~30 seconds)\n",
    "- The model loads into Colab's RAM (~10GB total memory required)\n",
    "- The inference engine compiles optimizations for your hardware\n",
    "- A warmup inference runs to ensure everything works\n",
    "\n",
    "**Step 3: Validation** (~5 seconds)\n",
    "- The toolkit runs a quick test to confirm the model can generate text\n",
    "- If successful, you'll see \"Engine initialized\" confirmation\n",
    "\n",
    "**‚è±Ô∏è Total Expected Time:**\n",
    "- First run: 5-10 minutes (mostly download time)\n",
    "- Subsequent runs: 30-45 seconds (loading only)\n",
    "\n",
    "**üíæ Colab Resource Check:**\n",
    "\n",
    "Free Colab provides:\n",
    "- **RAM:** 12-13GB (we need ~10GB for MedGemma)\n",
    "- **Disk:** 100GB+ (plenty for our 8GB model)\n",
    "- **Session duration:** 12 hours max, 90 minutes idle timeout\n",
    "\n",
    "**üö® Troubleshooting Common Issues:**\n",
    "\n",
    "| Problem | Likely Cause | Solution |\n",
    "|---------|-------------|----------|\n",
    "| \"Out of memory\" error | Other notebooks consuming RAM | Runtime ‚Üí Manage sessions ‚Üí Terminate others |\n",
    "| Download stalls at 99% | Network timeout | Restart runtime, rerun setup cell |\n",
    "| \"Model not found\" error | Hugging Face access issue | Check you're online; try again in 1-2 minutes |\n",
    "| Slow inference (>2 min/image) | CPU-only mode (no GPU allocated) | Runtime ‚Üí Change runtime type ‚Üí T4 GPU (if available) |\n",
    "\n",
    "**üí° Pro Tip:** If you're repeatedly restarting the runtime during experimentation, the model download is cached. You only pay the download cost once per Colab session."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "1eed34bb",
   "metadata": {},
   "source": [
    "## 2. Golden Path: First Assisted Read\n",
    "\n",
    "We'll analyse a sample chest X-ray while explicitly threading in the patient context you curated earlier."
   ]
  },
  {
   "cell_type": "code",
   "id": "10e6e9fc",
   "metadata": {},
   "source": [
    "sample_image_url = \"https://staticnew-prod.topdoctors.co.uk/files/Image/large/5ad0cd79-11d0-47b7-840b-123525bbab96.jpg\"\n",
    "clinical_focus = \"Monitor pneumonia resolution and cardiac workload\"\n",
    "question = \"Describe acute and chronic findings relevant to this patient's recovery plan.\"\n",
    "persona = \"radiologist\"\n",
    "\n",
    "print(\"1Ô∏è‚É£ Displaying imaging study:\")\n",
    "load_and_display_image(sample_image_url)\n",
    "\n",
    "prompt = build_radiology_prompt(\n",
    "    profile=patient_profile,\n",
    "    clinical_focus=clinical_focus,\n",
    "    question=question,\n",
    "    persona=persona,\n",
    ")\n",
    "print(\"\\n2Ô∏è‚É£ Prompt sent to MedGemma:\\n\")\n",
    "print(prompt)\n",
    "\n",
    "report = medgemma_engine.analyze(\n",
    "    image_path_or_url=sample_image_url,\n",
    "    prompt=prompt,\n",
    "    persona=persona,\n",
    ")\n",
    "\n",
    "print(\"\\n--- AI PRELIMINARY REPORT ---\")\n",
    "render_ai_report(report)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8d278f8a",
   "metadata": {},
   "source": [
    "Take a moment to compare the AI's commentary with your handover note. Where do they agree? Where should a clinician remain sceptical?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prz3vyy8m0d",
   "source": [
    "### üîç Critical Skill: Recognizing AI Hallucinations\n",
    "\n",
    "Now that you've seen your first AI-generated report, let's practice the most important safety skill: **detecting hallucinations and overconfident statements**.\n",
    "\n",
    "**What is a Hallucination?**\n",
    "\n",
    "In AI systems, a **hallucination** occurs when the model generates plausible-sounding content that isn't supported by the input data. In medical imaging, this means reporting findings that aren't actually visible in the image.\n",
    "\n",
    "**Common Hallucination Patterns in Radiology AI:**\n",
    "\n",
    "| Pattern | Example | Why It Happens |\n",
    "|---------|---------|----------------|\n",
    "| **Fabricated devices** | \"ET tube tip at the carina\" when no tube is present | Training data heavily featured intubated patients |\n",
    "| **Phantom pathology** | \"Small left pleural effusion\" on a normal film | Model associates clinical context with findings |\n",
    "| **Precise measurements** | \"Cardiac silhouette measures 14.2cm\" | Model generates numbers without actual measurement |\n",
    "| **Definitive diagnoses** | \"Pneumonia confirmed\" vs \"Opacity concerning for pneumonia\" | Overconfident language from training data |\n",
    "| **Temporal assumptions** | \"New infiltrate since prior study\" when no prior was provided | Model fabricates comparison context |\n",
    "\n",
    "**üß™ Quick Exercise: Audit the Report Above**\n",
    "\n",
    "Review the AI-generated report from Section 2. For each statement, classify it as:\n",
    "\n",
    "- ‚úÖ **Descriptive observation** (safe): \"The cardiac silhouette appears enlarged\"\n",
    "- ‚ö†Ô∏è **Interpretive finding** (verify): \"Findings consistent with pulmonary edema\"  \n",
    "- üö® **Definitive diagnosis** (escalate): \"Patient has congestive heart failure\"\n",
    "- ‚ùì **Unprovable claim** (hallucination risk): \"ETT tip is 3cm above the carina\" (if no ETT visible)\n",
    "\n",
    "**Which statements would you flag for radiologist verification?**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "19f8fd5d",
   "metadata": {},
   "source": [
    "## 3. Interactive Lab: Prompt Crafting & Personas\n",
    "\n",
    "You now control the imaging consult. Experiment with different focuses, questions, and personas while keeping track of what helps‚Äîor confuses‚Äîthe AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9lmfe30ghq",
   "source": "---\n\n### üéì What You've Learned So Far\n\n**Section 1-2 Recap:**\n- ‚úÖ Initialized a medical vision-language model in Colab\n- ‚úÖ Ran your first AI-assisted imaging analysis\n- ‚úÖ Learned to recognize hallucinations and verification requirements\n\n**Next:** Now you'll take control. In Section 3, you'll craft your own prompts, experiment with different personas, and discover what makes AI output clinically useful vs unreliable.\n\n**Estimated time for Section 3:** 30-40 minutes\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "38eb7b5c",
   "metadata": {},
   "source": "### Challenge 3.1: Structured Prompt Engineering\n\n**Clinical Scenario:**\n\nJordan's care team administered 3 liters of IV fluids during sepsis resuscitation. It's now Day 3, and the team is concerned about **fluid overload**. They need the AI to focus specifically on signs of pulmonary vascular congestion and pleural effusions.\n\n**Your Task:**\n\nCraft a new prompt that:\n1. Directs the AI's attention to **fluid status** (not general findings)\n2. Asks for **actionable observations** that inform diuresis decisions\n3. Maintains clinical context from Jordan's handover note\n\n**üìã Good Medical Prompt Checklist:**\n\nBefore you write your prompt, review these principles:\n\n- ‚úÖ **Specific clinical question** (not \"describe everything\")\n  - Good: \"Assess for pulmonary edema and pleural effusions\"\n  - Poor: \"Look at the chest X-ray\"\n\n- ‚úÖ **Relevant patient context** (from the handover note)\n  - Include: Recent fluid balance, current symptoms, treatment course\n  - Avoid: Irrelevant biographical details\n\n- ‚úÖ **Requests structured output**\n  - Ask for: Findings ‚Üí Impression ‚Üí Recommendations\n  - Avoid: Open-ended \"tell me what you see\"\n\n- ‚úÖ **Appropriate scope**\n  - Good: \"Describe findings concerning for volume overload\"\n  - Poor: \"Should we give furosemide?\" (asks AI to prescribe)\n\n**üí° Example Comparison:**\n\n| Aspect | ‚ùå Weak Prompt | ‚úÖ Strong Prompt |\n|--------|--------------|-----------------|\n| **Focus** | \"Analyze this chest X-ray\" | \"Assess for pulmonary vascular congestion in a patient with +3L fluid balance\" |\n| **Context** | None provided | \"Post-sepsis resuscitation, currently euvolemic per exam\" |\n| **Output** | Unstructured | \"Provide findings, impression, and suggest follow-up imaging if needed\" |\n| **Scope** | \"What should I do?\" | \"Describe radiographic findings that inform diuresis decisions\" |\n\n**üéØ Success Criteria:**\n\nYour prompt should produce a report that:\n- ‚úÖ Mentions specific findings related to fluid status (vascular markings, effusions, Kerley B lines)\n- ‚úÖ Connects observations back to the clinical question\n- ‚úÖ Uses appropriate hedging language (\"suggestive of\", \"may represent\") for ambiguous findings\n- ‚úÖ Flags uncertainties rather than guessing\n- ‚ùå Does NOT fabricate devices or measurements\n- ‚ùå Does NOT make treatment decisions\n\n**Now update the variables below and run the analysis:**"
  },
  {
   "cell_type": "code",
   "id": "e8b47bb4",
   "metadata": {},
   "source": "# TODO: Update these variables with your fluid-focused prompt\nexperiment_focus = \"Update this to focus on fluid overload assessment\"\nexperiment_question = \"Update this to ask for specific findings related to volume status\"\nexperiment_persona = \"radiologist\"\n\n# Build and display the prompt\nprompt = build_radiology_prompt(\n    profile=patient_profile,\n    clinical_focus=experiment_focus,\n    question=experiment_question,\n    persona=experiment_persona,\n)\nprint(\"=== YOUR PROMPT ===\")\nprint(prompt)\nprint(\"\\n\")\n\n# TODO: Remove the NotImplementedError and uncomment the code below when ready\nraise NotImplementedError(\"Challenge 3.1: Update the variables above, then remove this line and run the analysis.\")\n\n# Uncomment these lines after updating the prompt variables:\n# experiment_report = medgemma_engine.analyze(\n#     image_path_or_url=sample_image_url,\n#     prompt=prompt,\n#     persona=experiment_persona,\n# )\n#\n# print(\"=== AI REPORT ===\")\n# render_ai_report(experiment_report)\n#\n# print(\"\\nüß™ Self-Assessment: Review the success criteria above.\")\n# print(\"Did your prompt produce findings focused on fluid status?\")\n# print(\"Did the AI use appropriate hedging language?\")\n# print(\"Any hallucinations or overconfident statements to flag?\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "783ea033",
   "metadata": {},
   "source": [
    "### üìù Prompt Lab Log\n",
    "Record your trials here. What wording produced the clearest guidance?"
   ]
  },
  {
   "cell_type": "code",
   "id": "ad4af8bb",
   "metadata": {},
   "source": "prompt_lab_notes = '''\n# Prompt Experimentation Log\n\n**Instructions:** After each experiment (Challenges 3.1-3.3), record what you learned.\n\n| Experiment | Focus | Persona | Useful Takeaways | Cautions |\n|------------|-------|---------|------------------|----------|\n| **Example** | Assess cardiac silhouette size and pulmonary vasculature | Cardiologist | AI flagged possible cardiomegaly with CTR reference; mentioned pulmonary vascular redistribution | AI estimated CTR without actual measurement tools‚Äîwould need manual verification before reporting |\n| **Challenge 3.1** | <Your fluid overload focus> | <Your persona> | <What worked well?> | <What to verify or be skeptical about?> |\n| **Challenge 3.2** | <Your cardiology focus> | <Your persona> | <What changed with the new persona?> | <Any hallucinations or overconfidence?> |\n| **Challenge 3.3** | <Your custom image focus> | <Your persona> | <How did different image quality affect output?> | <Any misidentification of modality or view?> |\n\n---\n\n## Key Insights from Your Experiments\n\n**What makes a prompt effective?**\n- (Record your observations here after completing the challenges)\n\n**Which persona produced the most clinically useful output for Jordan's case?**\n- (Your answer here)\n\n**What findings did you consistently need to verify across all AI reports?**\n- (Common hallucination patterns you noticed)\n\n**If you were deploying this AI in a real radiology department, what safeguards would you require?**\n- (Your recommendations based on what you learned)\n\n---\n\nüí° **Pro Tip:** Save this log! These insights transfer to other clinical AI tools you'll encounter in practice.\n'''\nprint(prompt_lab_notes)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b5f76f87",
   "metadata": {},
   "source": "### Challenge 3.2: Persona Shifting & Priority Framing\n\n**The Power of Persona:**\n\nWhen you assign a \"persona\" to the AI, you're essentially asking it to emphasize the priorities and vocabulary of that specialty. Different specialists look at the same image through different clinical lenses.\n\n**How Personas Affect AI Output:**\n\n| Persona | Primary Focus | Typical Language | Common Priorities |\n|---------|--------------|------------------|-------------------|\n| **Radiologist** | Anatomical findings, differential diagnoses | \"Opacity,\" \"consolidation,\" \"silhouette sign\" | Comprehensive description, DDx |\n| **Cardiologist** | Cardiac silhouette, pulmonary vasculature | \"Cardiomegaly,\" \"vascular redistribution\" | Heart size, fluid status, device position |\n| **Intensivist** | Life-threatening findings, device placement | \"Emergent,\" \"critical,\" \"requires immediate\" | Pneumothorax, line placement, ARDS |\n| **Pulmonologist** | Parenchymal disease, airways | \"Infiltrate,\" \"bronchiectasis,\" \"hyperinflation\" | Infection, COPD, ILD patterns |\n\n**Clinical Scenario:**\n\nThe cardiology team is now rounding on Jordan. They're less concerned with pneumonia resolution (that's improving) and more focused on:\n- **Cardiac silhouette size** (concerned about cardiomegaly)\n- **Pulmonary vascular markings** (fluid status)\n- **Any signs of heart failure** that might complicate sepsis recovery\n\n**Your Task:**\n\n1. Update the persona to `\"cardiologist\"`\n2. Craft a focus statement that prioritizes cardiac concerns\n3. Ask a question that a cardiologist would care about (not a general radiologist)\n\n**üéØ Success Criteria:**\n\nThe AI report should:\n- ‚úÖ Emphasize cardiac findings over pulmonary details\n- ‚úÖ Use cardiology-specific terminology (CTR, vascular redistribution, etc.)\n- ‚úÖ Comment on findings relevant to heart failure assessment\n- ‚úÖ De-emphasize or briefly mention pneumonia (that's pulmonology's domain)\n\n**üí° Pro Tip:**\n\nThe best clinical prompts match the AI persona to the actual consult question. If cardiology is consulting for \"rule out cardiomegaly,\" use the cardiologist persona and frame the question accordingly."
  },
  {
   "cell_type": "code",
   "id": "23ca4398",
   "metadata": {},
   "source": "# TODO: Update these variables with a cardiology-focused prompt\npersona_focus = \"Update with cardiac-specific concerns (cardiac size, heart failure signs)\"\npersona_question = \"Update with a question a cardiologist would ask\"\npersona_label = \"cardiologist\"\n\n# Build and display the prompt\nprompt = build_radiology_prompt(\n    profile=patient_profile,\n    clinical_focus=persona_focus,\n    question=persona_question,\n    persona=persona_label,\n)\nprint(\"=== CARDIOLOGY-FOCUSED PROMPT ===\")\nprint(prompt)\nprint(\"\\n\")\n\n# TODO: Remove the NotImplementedError and uncomment the code below when ready\nraise NotImplementedError(\"Challenge 3.2: Update the variables above, then remove this line and run the analysis.\")\n\n# Uncomment these lines after updating the prompt variables:\n# persona_report = medgemma_engine.analyze(\n#     image_path_or_url=sample_image_url,\n#     prompt=prompt,\n#     persona=persona_label,\n# )\n#\n# print(\"=== CARDIOLOGY-FOCUSED REPORT ===\")\n# render_ai_report(persona_report)\n#\n# print(\"\\nüß™ Self-Assessment:\")\n# print(\"Compare this report to the radiologist report from Section 2.\")\n# print(\"- Which findings did the cardiologist persona emphasize?\")\n# print(\"- Which findings were de-emphasized or omitted?\")\n# print(\"- Did the language and priorities shift appropriately?\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "183e757c",
   "metadata": {},
   "source": "### Challenge 3.3: Bring Your Own Image & Test Edge Cases\n\n**Time to Stress-Test the AI:**\n\nNow that you understand prompt engineering and persona selection, let's see how the AI handles different types of images. This challenge teaches you to recognize when AI performs well vs. when you should be skeptical.\n\n**Your Task:**\n\n1. **Find a public medical image** (chest X-ray, CT, or other modality) that's relevant to Jordan's case or a similar clinical scenario\n2. **Adjust your prompt** to match the image and clinical context\n3. **Run the full workflow** and critically evaluate the output\n\n**üì∏ Where to Find Medical Images:**\n\n- **Radiopaedia:** https://radiopaedia.org/ (free cases with teaching files)\n- **Open-i NIH:** https://openi.nlm.nih.gov/ (biomedical image search)\n- **Wikipedia Commons:** Medical imaging category (public domain)\n\n**‚ö†Ô∏è Image Selection Tips:**\n\nDifferent image types will challenge the AI in different ways:\n\n| Image Type | What It Tests | Expected AI Performance |\n|------------|--------------|------------------------|\n| **High-quality PA chest X-ray** | Baseline capability | Should perform well |\n| **Portable/AP chest X-ray** | Lower quality tolerance | May struggle with subtle findings |\n| **Lateral chest X-ray** | View interpretation | May describe as if it's PA view (hallucination!) |\n| **CT scan** | Modality recognition | May confuse findings or use wrong terminology |\n| **Pediatric image** | Age-specific findings | May apply adult reference ranges |\n| **Image with poor exposure** | Artifacts & quality issues | Higher hallucination risk |\n\n**üéØ Learning Objectives:**\n\nBy running this challenge, you'll discover:\n- ‚úÖ Which image qualities produce reliable AI output\n- ‚úÖ How the AI handles modality and view variations\n- ‚úÖ When you should have lower confidence in AI findings\n- ‚úÖ The importance of matching clinical context to image type\n\n**üí° Suggested Experiment:**\n\nTry two contrasting images:\n1. A **high-quality teaching file** (optimal conditions for AI)\n2. A **portable/bedside film** (realistic ICU conditions)\n\nCompare the AI's confidence, specificity, and accuracy between the two."
  },
  {
   "cell_type": "code",
   "id": "8dd59db2",
   "metadata": {},
   "source": "# TODO: Update these variables with your chosen image and customized prompt\nyour_image_url = \"https://example.com/your-chosen-image.jpg\"  # Replace with actual URL\nyour_focus = \"Tailor this focus to match the clinical scenario for your chosen image\"\nyour_question = \"What specific clinical question does this image need to answer?\"\nyour_persona = \"radiologist\"  # Or cardiologist, pulmonologist, etc.\n\n# Display the image first\nprint(\"=== YOUR CHOSEN IMAGE ===\")\nload_and_display_image(your_image_url)\n\n# Build and display the prompt\nprompt = build_radiology_prompt(\n    profile=patient_profile,\n    clinical_focus=your_focus,\n    question=your_question,\n    persona=your_persona,\n)\nprint(\"\\n=== YOUR PROMPT ===\")\nprint(prompt)\nprint(\"\\n\")\n\n# TODO: Remove the NotImplementedError and uncomment the code below when ready\nraise NotImplementedError(\"Challenge 3.3: Update the variables above (especially your_image_url), then remove this line.\")\n\n# Uncomment these lines after updating all variables:\n# your_report = medgemma_engine.analyze(\n#     image_path_or_url=your_image_url,\n#     prompt=prompt,\n#     persona=your_persona,\n# )\n#\n# print(\"=== AI REPORT FOR YOUR IMAGE ===\")\n# render_ai_report(your_report)\n#\n# print(\"\\nüß™ Critical Evaluation Checklist:\")\n# print(\"[ ] Did the AI correctly identify the image modality (X-ray vs CT vs other)?\")\n# print(\"[ ] Did the AI correctly identify the view (PA vs AP vs lateral)?\")\n# print(\"[ ] Are there any findings the AI reported that you can't see in the image?\")\n# print(\"[ ] Did image quality affect the AI's confidence or specificity?\")\n# print(\"[ ] Would you trust this report enough to escalate to the attending, or does it need revision?\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0e849ac8",
   "metadata": {},
   "source": [
    "## 4. Advanced Mission: Analyse Your Own Data\n",
    "\n",
    "If a remote URL fails‚Äîor you want to test a de-identified case from your institution‚Äîupload it directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i15m0f4rhj",
   "source": "---\n\n### üéì Section 3 Checkpoint\n\n**What You've Practiced:**\n- ‚úÖ Prompt engineering for specific clinical questions\n- ‚úÖ Persona shifting to match consult scenarios\n- ‚úÖ Evaluating AI performance across different image types\n\n**Key Insight:** The quality of AI output depends heavily on how you frame the question and what context you provide.\n\n**Next:** Section 4 shows you how to work with locally uploaded images (useful for de-identified institutional cases).\n\n**Estimated time for Section 4:** 10-15 minutes\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "0b0b6826",
   "metadata": {},
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"Please upload an image file (e.g., .jpg, .png).\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "if uploaded:\n",
    "    local_image_path = next(iter(uploaded))\n",
    "    print(f\"Displaying uploaded image: {local_image_path}\")\n",
    "    load_and_display_image(local_image_path)\n",
    "\n",
    "    local_focus = \"Describe why you captured this image\"\n",
    "    local_question = \"What actionable insights do you need before rounds?\"\n",
    "\n",
    "    prompt = build_radiology_prompt(\n",
    "        profile=patient_profile,\n",
    "        clinical_focus=local_focus,\n",
    "        question=local_question,\n",
    "        persona=\"radiologist\",\n",
    "    )\n",
    "\n",
    "    local_report = medgemma_engine.analyze(\n",
    "        image_path_or_url=local_image_path,\n",
    "        prompt=prompt,\n",
    "        persona=\"radiologist\",\n",
    "    )\n",
    "\n",
    "    print(\"--- AI REPORT (LOCAL IMAGE) ---\")\n",
    "    render_ai_report(local_report)\n",
    "else:\n",
    "    raise FileNotFoundError(\"Upload cancelled. Please rerun the cell and choose an image.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d0739019",
   "metadata": {},
   "source": [
    "## 5. Extension: Benchmarking Against the Competition\n",
    "\n",
    "MedVision's R&D team wants a head-to-head comparison between MedGemma (production) and LLaVA (experimental). Keep your evaluation clinically grounded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bpg0wwea43f",
   "source": "---\n\n### üéì Section 4 Checkpoint\n\n**What You've Practiced:**\n- ‚úÖ Working with locally uploaded images (not just URLs)\n- ‚úÖ Adapting prompts to custom clinical scenarios\n\n**Next:** Section 5 is the capstone‚Äîyou'll compare two competing AI models using a clinical decision framework and make a deployment recommendation to the MedVision leadership team.\n\nThis is where you synthesize everything you've learned about prompt engineering, hallucination detection, and clinical evaluation.\n\n**Estimated time for Section 5:** 20-30 minutes\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "30e6704e",
   "metadata": {},
   "source": [
    "### Benchmark Protocol\n",
    "1. Form a hypothesis: where do you expect MedGemma to outperform LLaVA?\n",
    "2. Keep the patient context, image, and prompt identical across engines.\n",
    "3. Evaluate structure, accuracy, and bedside usefulness.\n",
    "4. Capture findings in the comparison log."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mjqqu53vgkt",
   "source": "### How to Evaluate Medical AI: A Clinical Decision Framework\n\nWhen comparing AI models for clinical deployment, **technical metrics alone aren't enough**. A model with impressive benchmark scores might still be unsafe if it hallucinates critical findings or uses overconfident language.\n\nUse this clinical evaluation rubric to assess both models:\n\n#### **Clinical Evaluation Rubric**\n\n| Criterion | Weight | What to Assess | Example |\n|-----------|--------|----------------|---------|\n| **üî¥ Accuracy** | Critical | Does the model correctly identify findings that ARE present? | Model identifies consolidation visible in RLL |\n| **üî¥ Safety** | Critical | Does the model avoid hallucinating findings that AREN'T present? | Model doesn't report ETT when none is visible |\n| **üü° Specificity** | Important | Does the model use precise anatomical/radiological terminology? | \"RLL consolidation\" vs \"white spot in lung\" |\n| **üü° Actionability** | Important | Do findings inform clinical decisions? | \"Small effusion, clinical correlation advised\" vs \"Some fluid maybe\" |\n| **üü° Appropriate Uncertainty** | Important | Does the model say \"unclear\" when image quality limits assessment? | \"Poor inspiration limits evaluation of bases\" |\n| **üü° Structured Output** | Important | Is the report organized (Findings ‚Üí Impression ‚Üí Recommendations)? | Clear sections vs stream of consciousness |\n| **üü¢ Formatting** | Nice-to-have | Is the output scannable and easy to read? | Bullet points, clear paragraphs |\n\n**Priority Order:**\n1. **Safety first:** A model that hallucinates less is better than one with prettier formatting\n2. **Clinical utility:** Reports must be actionable for the care team\n3. **Usability:** Structure and formatting enhance (but don't replace) clinical value\n\n---\n\n#### **How to Make Your Deployment Recommendation**\n\nAfter running both models, ask yourself:\n\n**For Safety & Accuracy (Most Important):**\n- Which model made fewer safety-critical errors (missed findings, hallucinations)?\n- Which model was more conservative with uncertainty (appropriately hedged)?\n- If you had to escalate ONE report to the attending, which would it be?\n\n**For Clinical Utility:**\n- Which model provided findings that directly address the clinical question?\n- Which model connected findings to patient context more effectively?\n- Which model's output would save the radiologist the most time?\n\n**For Trust & Transparency:**\n- Which model's outputs were more consistent across different prompts?\n- Which model was easier to \"steer\" with persona and focus adjustments?\n- If an error occurred, which model's mistake would be easier to catch?\n\n**Your Final Recommendation Should State:**\n1. **Which model to deploy** (MedGemma or LLaVA)\n2. **Why** (specific strengths observed)\n3. **What safeguards are required** (human review protocol, specific findings to always verify)\n4. **What limitations remain** (edge cases where neither model is reliable)\n\n---\n\n**Now let's run the benchmark with these criteria in mind.**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "163d269b",
   "metadata": {},
   "source": "from medvision_toolkit import initialize_llava_engine\n\n# Step 1: Initialize the experimental LLaVA engine\nprint(\"--- Initialising LLaVA Experimental Engine ---\")\nprint(\"‚è±Ô∏è This may take 3-5 minutes on first run (downloading model weights)...\\n\")\n\n# TODO: Remove the NotImplementedError below when ready to run the benchmark\nraise NotImplementedError(\"Challenge 5: Remove this line to initialize LLaVA and run the benchmark comparison.\")\n\n# Uncomment the code below after removing the NotImplementedError:\n#\n# llava_engine = initialize_llava_engine()\n# print(\"‚úÖ LLaVA engine ready.\\n\")\n#\n# # Step 2: Define a test case (same image and prompt for both models)\n# test_image_url = \"https://upload.wikimedia.org/wikipedia/commons/c/c8/Chest_Xray_PA_3-8-2010.png\"\n# test_focus = \"Assess for interval changes, urgent safety findings, and signs of infection\"\n# test_question = \"Provide a structured report with findings, impression, and follow-up recommendations.\"\n# test_persona = \"radiologist\"\n#\n# # Build the standardized prompt\n# prompt = build_radiology_prompt(\n#     profile=patient_profile,\n#     clinical_focus=test_focus,\n#     question=test_question,\n#     persona=test_persona,\n# )\n#\n# print(\"=== BENCHMARK TEST CASE ===\")\n# print(f\"Image: {test_image_url}\")\n# load_and_display_image(test_image_url)\n# print(\"\\n=== STANDARDIZED PROMPT (Same for Both Models) ===\")\n# print(prompt)\n# print(\"\\n\" + \"=\"*80)\n#\n# # Step 3: Run MedGemma analysis\n# print(\"\\nüî¨ Running MedGemma analysis...\")\n# print(\"(This may take 30-60 seconds)\\n\")\n# medgemma_report = medgemma_engine.analyze(\n#     image_path_or_url=test_image_url,\n#     prompt=prompt,\n#     persona=test_persona,\n# )\n#\n# # Step 4: Run LLaVA analysis\n# print(\"\\nüî¨ Running LLaVA analysis...\")\n# print(\"(This may take 30-60 seconds)\\n\")\n# llava_report = llava_engine.analyze(\n#     image_path_or_url=test_image_url,\n#     prompt=prompt,\n# )\n#\n# # Step 5: Display both reports side-by-side\n# print(\"\\n\" + \"=\"*80)\n# print(\"üìä BENCHMARK RESULTS\")\n# print(\"=\"*80)\n#\n# print(\"\\n--- [PRODUCTION] MedGemma Report ---\")\n# print(\"-\" * 80)\n# render_ai_report(medgemma_report)\n#\n# print(\"\\n\\n--- [EXPERIMENTAL] LLaVA Report ---\")\n# print(\"-\" * 80)\n# render_ai_report(llava_report)\n#\n# print(\"\\n\" + \"=\"*80)\n# print(\"\\nüß™ Now complete the benchmark comparison log above.\")\n# print(\"Use the clinical evaluation rubric to assess both models systematically.\")\n# print(\"Focus on: Safety > Accuracy > Actionability > Structure\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5aa8634a",
   "metadata": {},
   "source": [
    "### üìù Benchmark Comparison Log\n",
    "Summarise the relative strengths, limitations, and deployment decision."
   ]
  },
  {
   "cell_type": "code",
   "id": "17e8d458",
   "metadata": {},
   "source": "benchmark_notes = '''\n# Model Comparison & Deployment Decision\n\n**Instructions:** After running both MedGemma and LLaVA on the same test case, complete this evaluation.\n\n---\n\n## Side-by-Side Comparison\n\n| Criterion | MedGemma | LLaVA | Winner |\n|-----------|----------|-------|--------|\n| **üî¥ Accuracy** | <Correctly identified findings?> | <Correctly identified findings?> | <Which was more accurate?> |\n| **üî¥ Safety (No Hallucinations)** | <Any fabricated findings?> | <Any fabricated findings?> | <Which hallucinated less?> |\n| **üü° Specificity** | <Precise terminology?> | <Precise terminology?> | <Which was more specific?> |\n| **üü° Actionability** | <Clinically useful?> | <Clinically useful?> | <Which was more actionable?> |\n| **üü° Appropriate Uncertainty** | <Hedged appropriately?> | <Hedged appropriately?> | <Which handled uncertainty better?> |\n| **üü° Structure** | <Well-organized report?> | <Well-organized report?> | <Which had better structure?> |\n\n---\n\n## Detailed Observations\n\n### MedGemma Strengths:\n- (What did MedGemma do particularly well?)\n\n### MedGemma Limitations:\n- (What were MedGemma's weaknesses or errors?)\n\n### LLaVA Strengths:\n- (What did LLaVA do particularly well?)\n\n### LLaVA Limitations:\n- (What were LLaVA's weaknesses or errors?)\n\n---\n\n## Deployment Recommendation\n\n**Recommended Model for Clinical Deployment:** <MedGemma / LLaVA / Neither>\n\n**Rationale:**\n- (Why did you choose this model? Focus on safety and clinical utility)\n\n**Required Safeguards:**\n1. (What human oversight is required?)\n2. (Which findings must always be verified by a radiologist?)\n3. (What patient populations or image types should be excluded?)\n\n**Remaining Concerns:**\n- (What would need to improve before you'd trust this AI in practice?)\n- (Are there edge cases where neither model is reliable?)\n\n---\n\n## Key Takeaway\n\nIf you had to summarize your learning from this benchmark in one sentence for the MedVision leadership team:\n\n**\"<Your one-sentence recommendation here>\"**\n\n---\n\n**Example Completed Entry:**\n\nModel: MedGemma\nStrengths: Conservative language (\"may represent\"), good structure, identified major findings\nLimitations: Slightly verbose, estimated cardiac measurements without tools\nWinner for safety-critical deployment: MedGemma (fewer hallucinations)\n\n'''\nprint(benchmark_notes)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fe886ab8",
   "metadata": {},
   "source": [
    "### Solution (Click to Expand)\n",
    "\n",
    "Run this cell only after attempting the benchmark yourself."
   ]
  },
  {
   "cell_type": "code",
   "id": "67904e60",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "solution"
    ]
   },
   "source": [
    "\n",
    "print(\"--- Initialising Llava Experimental Engine ---\")\n",
    "llava_engine = initialize_llava_engine()\n",
    "\n",
    "test_image_url = \"https://upload.wikimedia.org/wikipedia/commons/c/c8/Chest_Xray_PA_3-8-2010.png\"\n",
    "test_focus = \"Assess interval changes and urgent safety findings\"\n",
    "test_question = \"Provide a structured report with impression and follow-up recommendations.\"\n",
    "\n",
    "prompt = build_radiology_prompt(\n",
    "    profile=patient_profile,\n",
    "    clinical_focus=test_focus,\n",
    "    question=test_question,\n",
    "    persona=\"radiologist\",\n",
    ")\n",
    "\n",
    "print(\"\\nRunning MedGemma analysis...\")\n",
    "medgemma_report = medgemma_engine.analyze(\n",
    "    image_path_or_url=test_image_url,\n",
    "    prompt=prompt,\n",
    "    persona=\"radiologist\",\n",
    ")\n",
    "\n",
    "print(\"\\nRunning LLaVA analysis...\")\n",
    "llava_report = llava_engine.analyze(\n",
    "    image_path_or_url=test_image_url,\n",
    "    prompt=prompt,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"--- [PRODUCTION] MedGemma Report ---\")\n",
    "print(\"=\" * 80)\n",
    "print(medgemma_report)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"--- [EXPERIMENTAL] LLaVA Report ---\")\n",
    "print(\"=\" * 80)\n",
    "print(llava_report)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "10515609",
   "metadata": {},
   "source": "## 6. Clinical Debrief: From Lab to Practice\n\n### What You've Accomplished\n\nOver the past two notebooks, you've built a complete clinical AI workflow:\n\n**Notebook 01: Python Fundamentals**\n- ‚úÖ Structured patient data using variables, lists, and dictionaries\n- ‚úÖ Automated triage logic with loops and conditionals\n- ‚úÖ Generated handover notes from typed data structures\n\n**Notebook 02: AI-Assisted Radiology**\n- ‚úÖ Understood how vision-language models work (and their limitations)\n- ‚úÖ Initialized and operated a medical VLM in a resource-constrained environment\n- ‚úÖ Crafted prompts that ground AI analysis in clinical context\n- ‚úÖ Recognized hallucinations and applied safety verification protocols\n- ‚úÖ Evaluated competing models using clinical decision criteria\n- ‚úÖ Made an evidence-based deployment recommendation\n\n**This is no small feat.** You've gone from zero Python experience to operating cutting-edge AI tools with clinical discipline.\n\n---\n\n### Critical Insights to Take Forward\n\n#### **On AI Capabilities:**\n\n**What AI Does Well:**\n- Rapid pattern recognition across large imaging datasets\n- Drafting preliminary reports that save radiologist time\n- Flagging potential findings for human review (triage)\n- Maintaining consistency in terminology and structure\n\n**What AI Struggles With:**\n- Rare or unusual pathologies not well-represented in training data\n- Precise measurements (angles, volumes, dimensions) without measurement tools\n- Integrating clinical context beyond what's explicitly stated in the prompt\n- Recognizing when image quality limits assessment\n\n**The Bottom Line:** AI is a powerful assistant, not a replacement. It excels at the 80% of routine cases but needs human expertise for the nuanced 20%.\n\n---\n\n#### **On Prompt Engineering:**\n\n**What You Learned:**\n\n1. **Specificity Matters:**\n   - \"Assess for pulmonary edema\" > \"Look at the chest X-ray\"\n   - The more specific your clinical question, the more focused the AI response\n\n2. **Context Transforms Output:**\n   - Providing patient history changes what the AI considers clinically relevant\n   - The same image analyzed with different context produces different findings\n\n3. **Persona Framing Works:**\n   - Assigning a specialist role (cardiologist vs radiologist) shifts priorities\n   - Match the persona to the actual consult question for best results\n\n4. **Structure Improves Usability:**\n   - Asking for \"Findings ‚Üí Impression ‚Üí Recommendations\" produces actionable reports\n   - Open-ended prompts lead to rambling, less useful outputs\n\n**Transferable Skill:** These prompt engineering principles apply to all clinical AI tools, not just radiology.\n\n---\n\n#### **On Safety & Verification:**\n\n**Hallucinations Are Common:**\n- All VLMs occasionally report findings that aren't present\n- Devices, measurements, and temporal comparisons are high-risk areas\n- The more specific/confident the AI sounds, the more skeptical you should be\n\n**Trust, But Verify:**\n- Every AI finding requires correlation with source images\n- Critical findings (pneumothorax, free air, PE) require immediate human review\n- Document that AI was used as a decision support tool, not a diagnostic authority\n\n**Scope of Practice:**\n- As students/junior doctors, you're learning to *consume* AI outputs critically\n- Interpreting studies and making diagnoses remains the radiologist's domain\n- Know when to escalate vs when to trust preliminary AI reads\n\n---\n\n### Reflection Exercise\n\nBefore closing this notebook, take 5 minutes to answer these questions for yourself. You can write your responses in a new markdown cell below, or in your personal notes.\n\n**1. Clinical Judgment:**\n\n*\"Under what circumstances would you trust this AI enough to flag a case as 'likely normal' and deprioritize it in the reading queue? What safeguards would you require?\"*\n\n**2. Workflow Integration:**\n\n*\"Where in the care pathway would AI-assisted reads add the most value at your institution? (Screening? Overnight triage? Teaching files? Quality assurance?)\"*\n\n**3. Ethical Responsibility:**\n\n*\"If an AI-assisted read missed a finding that a radiologist would have caught, who is responsible? How does this affect your comfort level using these tools?\"*\n\n**4. Personal Learning:**\n\n*\"What surprised you most about working with medical AI? What remains concerning or unclear?\"*\n\n---\n\n### Next Steps in Your AI-Augmented Practice\n\n#### **In This Course (Optional Extensions):**\n\n- **Stress-test the models:** Repeat the benchmark with your own de-identified cases or challenging public datasets\n- **Build a prompt library:** Create reusable prompt templates for common clinical scenarios (PE protocol, pneumonia follow-up, post-op chest)\n- **Share your findings:** Present your benchmark results to your learning group or clinical supervisor\n\n#### **In Your Clinical Training:**\n\n- **Seek out AI tools:** Ask your radiology, pathology, and cardiology departments what AI systems they're using\n- **Observe integration:** Watch how attendings incorporate AI into their workflow (Do they review every AI flag? Ignore certain outputs?)\n- **Advocate for transparency:** If AI influences a decision about your patient, make sure it's documented in the chart\n\n#### **Stay Current:**\n\n- **Follow the literature:** Medical AI is evolving rapidly; what's true today may change in 6 months\n- **Attend AI grand rounds:** Many academic centers now host regular sessions on clinical AI\n- **Engage with policy:** Regulatory frameworks for medical AI are being written *now*; your generation will inherit (or revise) them\n\n---\n\n### Final Thought: Stay Curious, Stay Critical\n\nYou now have the foundation to be a **thoughtful consumer and collaborator** with medical AI. You understand:\n\n- How these systems work (pattern recognition, not reasoning)\n- Where they excel (routine cases, rapid triage)\n- Where they fail (hallucinations, rare pathology)\n- How to evaluate them (safety > accuracy > utility)\n\n**The technology will evolve.** Models will get better, faster, and more specialized. But your responsibility to **verify, question, and prioritize patient safety** will not change.\n\nAI is a tool. Like a stethoscope, it's only as good as the clinician wielding it.\n\n**You've earned the right to wield it well. Congratulations on completing Project MedVision.**\n\n---\n\n### Optional: Document Your Work\n\nIf you'd like to save your experiments for future reference:\n\n1. **Download this notebook:** File ‚Üí Download ‚Üí Download .ipynb\n2. **Save your logs:** Copy your prompt lab notes and benchmark notes to a personal document\n3. **Capture key examples:** Screenshot particularly good or bad AI outputs for teaching cases\n\nYour future self (or your med school study group) will thank you."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
